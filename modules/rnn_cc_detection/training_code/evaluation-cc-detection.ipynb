{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook to test the effectiveness of the CC detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 16:34:31.767647: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-20 16:34:31.767718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-20 16:34:31.769822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-20 16:34:31.784120: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 16:34:33.623076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values\n",
    "csvfile = '../datasets/all_datasets_raw.tsv'\n",
    "min_letters = 5\n",
    "max_letters = 500\n",
    "take_last_num = lambda x: x[: max_letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "        csvfile,\n",
    "        delimiter=\"|\",\n",
    "        names=[\"note\", \"label\", \"model_id\", \"state\"],\n",
    "        skipinitialspace=True,\n",
    "        converters={\"state\": take_last_num},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset\n",
    "df.dropna(axis=0, how=\"any\", inplace=True)\n",
    "df.drop(axis=1, columns=[\"note\", \"model_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the strings of letters with less than a certain amount\n",
    "indexNames = df[df[\"state\"].str.len() < min_letters].index\n",
    "df.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to the dataframe with the label. The label is 'Normal' for the normal data and 'Malcious' for the malware data\n",
    "df.loc[df.label.str.contains(\"Normal\"), \"label\"] = \"Normal\"\n",
    "df.loc[df.label.str.contains(\"Botnet\"), \"label\"] = \"Malicious\"\n",
    "df.loc[df.label.str.contains(\"Malware\"), \"label\"] = \"Malicious\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2258671/31430727.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.label = df.label.replace(\"Normal\", 0)\n"
     ]
    }
   ],
   "source": [
    "df.label = df.label.replace(\"Malicious\", 1)\n",
    "df.label = df.label.replace(\"Normal\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 letters in total. From letter index 0.0 to letter index 49.0.\n"
     ]
    }
   ],
   "source": [
    "# Convert each of the stratosphere letters to an integer. There are 50\n",
    "vocabulary = list(\"abcdefghiABCDEFGHIrstuvwxyzRSTUVWXYZ1234567890,.+*\")\n",
    "int_of_letters = {}\n",
    "for i, letter in enumerate(vocabulary):\n",
    "    int_of_letters[letter] = float(i)\n",
    "print( f\"There are {len(int_of_letters)} letters in total. From letter index {min(int_of_letters.values())} to letter index {max(int_of_letters.values())}.\")\n",
    "vocabulary_size = len(int_of_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the letters in the state to an integer representing it uniquely. We 'encode' them.\n",
    "df[\"state\"] = df[\"state\"].apply(lambda x: [[int_of_letters[i]] for i in x])\n",
    "# So far, only 1 feature per letter\n",
    "features_per_sample = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70      [[44.0], [44.0], [45.0], [17.0], [17.0], [49.0...\n",
       "71      [[36.0], [36.0], [47.0], [27.0], [47.0], [27.0...\n",
       "73               [[44.0], [41.0], [47.0], [14.0], [47.0]]\n",
       "75      [[41.0], [41.0], [47.0], [35.0], [47.0], [26.0...\n",
       "76      [[44.0], [44.0], [47.0], [35.0], [47.0], [26.0...\n",
       "                              ...                        \n",
       "7609    [[44.0], [44.0], [47.0], [17.0], [47.0], [17.0...\n",
       "7610    [[44.0], [44.0], [47.0], [17.0], [47.0], [26.0...\n",
       "7616             [[43.0], [44.0], [46.0], [35.0], [48.0]]\n",
       "7625    [[39.0], [39.0], [47.0], [27.0], [48.0], [21.0...\n",
       "7630    [[43.0], [43.0], [47.0], [7.0], [47.0], [7.0],...\n",
       "Name: state, Length: 2228, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2228 outtuples\n"
     ]
    }
   ],
   "source": [
    "# Convert the data into the appropriate shape\n",
    "# x_data is a list of lists. The 1st dimension is the outtuple, the second the letter. Each letter is now an int value. shape=(num_outuples, features_per_sample)\n",
    "x_data = df[\"state\"].to_numpy()\n",
    "print(f\"There are {len(x_data)} outtuples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2228 labels\n"
     ]
    }
   ],
   "source": [
    "# y_data is a list of ints that are 0 or 1. One integer per outtupple. shape=(num_outuples, 1)\n",
    "y_data = df[\"label\"].to_numpy()\n",
    "print(f\"There are {len(y_data)} labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max len of the letters in all outtuples is: 500\n"
     ]
    }
   ],
   "source": [
    "# Search the sample with max len in the training. It should be already cuted by the csv_read function to a max. Here we just check\n",
    "max_length_of_outtupple = max([len(sublist) for sublist in df.state.to_list()])\n",
    "print(f\"The max len of the letters in all outtuples is: {max_length_of_outtupple}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data type <class 'numpy.ndarray'> of shape (2228,). x_data[0] type is <class 'list'>\n",
      "x_data[0] is [[44.0], [44.0], [45.0], [17.0], [17.0], [49.0], [26.0], [49.0], [45.0], [35.0], [45.0], [17.0]]\n"
     ]
    }
   ],
   "source": [
    "# Here x_data is a array of lists [[]]\n",
    "print(f\"x_data type {type(x_data)} of shape {x_data.shape}. x_data[0] type is {type(x_data[0])}\")\n",
    "print(f\"x_data[0] is {x_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded_x_data is of type <class 'numpy.ndarray'>, of shape (2228, 500, 1). padded_x_data[0] type is <class 'numpy.ndarray'>. Shape of second list is (500, 1)\n"
     ]
    }
   ],
   "source": [
    "# Padding.\n",
    "# Since not all outtuples have the same amount of letters, we need to add padding at the end\n",
    "# Transforms the list to a 2D Numpy array of shape (num_samples, num_timesteps)\n",
    "# num_timesteps is either the maxlen argument if provided, or the length of the longest sequence otherwise.\n",
    "# Sequences that are shorter than num_timesteps are padded with value at the end.\n",
    "# padding: 'pre' or 'post': pad either before or after each sequence.\n",
    "# truncating: 'pre' or 'post': remove values from sequences larger than maxlen, either at the beginning or at the end of the sequences.\n",
    "\n",
    "# If the input are integers\n",
    "padded_x_data = pad_sequences(\n",
    "    x_data, maxlen=max_length_of_outtupple, padding=\"post\"\n",
    ")\n",
    "print(\n",
    "        f\"padded_x_data is of type {type(padded_x_data)}, of shape {padded_x_data.shape}. padded_x_data[0] type is {type(padded_x_data[0])}. Shape of second list is {padded_x_data[0].shape}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_data = padded_x_data\n",
    "train_y_data = y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have as shape: Num of samples: 2228, Num of letters per sample (timesteps): 500, each letter has 1 values. The input shape is (500, 1)\n"
     ]
    }
   ],
   "source": [
    "num_outtuples = train_x_data.shape[0]  # number_of_outtuples in general\n",
    "\n",
    "# In the case of hot-encoding, the amount of features per letter per sample, is 50, which is the vocabulary size\n",
    "# features_per_sample = vocabulary_size # amount of positions of the hot encoding (50 letters, so 50)\n",
    "# print(f'We have as input shape: {num_outtuples}, {max_length_of_outtupple}, {features_per_sample}')\n",
    "# input_shape = (max_length_of_outtupple, features_per_sample)\n",
    "\n",
    "# In the case of not using hot-encoding, the amount of features per sample is 1, because we only have one value\n",
    "# The amount of time steps is the amount of letters, since one letter is one time step, which is the amount of letters max, which 500\n",
    "timesteps = max_length_of_outtupple\n",
    "input_shape = (timesteps, features_per_sample)\n",
    "print(\n",
    "    f\"We have as shape: Num of samples: {num_outtuples}, Num of letters per sample (timesteps): {timesteps}, each letter has {features_per_sample} values. The input shape is {input_shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model of RNN\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.Embedding(vocabulary_size, 16, mask_zero=True))\n",
    "# GRU is the main RNN layer, inputs: A 3D tensor, with shape [batch, timesteps, feature]\n",
    "model.add(\n",
    "    layers.Bidirectional(\n",
    "        layers.GRU(32, return_sequences=False), merge_mode=\"concat\"\n",
    "    )\n",
    ")\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "# Fully connected layer with 1 neuron output\n",
    "# Final output value between 0 and 1 as probability\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, momentum=0.05),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "21/21 [==============================] - 22s 524ms/step - loss: 0.6924 - accuracy: 0.5397 - val_loss: 0.6949 - val_accuracy: 0.3946\n",
      "Epoch 2/500\n",
      "21/21 [==============================] - 7s 330ms/step - loss: 0.6845 - accuracy: 0.7501 - val_loss: 0.6983 - val_accuracy: 0.3184\n",
      "Epoch 3/500\n",
      "21/21 [==============================] - 7s 333ms/step - loss: 0.6750 - accuracy: 0.7556 - val_loss: 0.7018 - val_accuracy: 0.3184\n",
      "Epoch 4/500\n",
      "21/21 [==============================] - 7s 325ms/step - loss: 0.6669 - accuracy: 0.7581 - val_loss: 0.7062 - val_accuracy: 0.3184\n",
      "Epoch 5/500\n",
      "21/21 [==============================] - 7s 324ms/step - loss: 0.6558 - accuracy: 0.7576 - val_loss: 0.7103 - val_accuracy: 0.3184\n",
      "Epoch 6/500\n",
      "21/21 [==============================] - 7s 326ms/step - loss: 0.6464 - accuracy: 0.7566 - val_loss: 0.7142 - val_accuracy: 0.3184\n",
      "Epoch 7/500\n",
      "21/21 [==============================] - 7s 323ms/step - loss: 0.6359 - accuracy: 0.7576 - val_loss: 0.7194 - val_accuracy: 0.3184\n",
      "Epoch 8/500\n",
      "21/21 [==============================] - 7s 322ms/step - loss: 0.6256 - accuracy: 0.7571 - val_loss: 0.7260 - val_accuracy: 0.3184\n",
      "Epoch 9/500\n",
      "21/21 [==============================] - 7s 334ms/step - loss: 0.6146 - accuracy: 0.7566 - val_loss: 0.7333 - val_accuracy: 0.3184\n",
      "Epoch 10/500\n",
      "21/21 [==============================] - 7s 324ms/step - loss: 0.5969 - accuracy: 0.7576 - val_loss: 0.7387 - val_accuracy: 0.3184\n",
      "Epoch 11/500\n",
      "21/21 [==============================] - 7s 325ms/step - loss: 0.5858 - accuracy: 0.7561 - val_loss: 0.7457 - val_accuracy: 0.3184\n",
      "Epoch 12/500\n",
      "21/21 [==============================] - 7s 335ms/step - loss: 0.5716 - accuracy: 0.7561 - val_loss: 0.7561 - val_accuracy: 0.3184\n",
      "Epoch 13/500\n",
      "21/21 [==============================] - 7s 330ms/step - loss: 0.5574 - accuracy: 0.7581 - val_loss: 0.7614 - val_accuracy: 0.3184\n",
      "Epoch 14/500\n",
      "21/21 [==============================] - 7s 323ms/step - loss: 0.5449 - accuracy: 0.7561 - val_loss: 0.7665 - val_accuracy: 0.3184\n",
      "Epoch 15/500\n",
      "21/21 [==============================] - 7s 326ms/step - loss: 0.5314 - accuracy: 0.7586 - val_loss: 0.7740 - val_accuracy: 0.3184\n",
      "Epoch 16/500\n",
      "21/21 [==============================] - 7s 329ms/step - loss: 0.5156 - accuracy: 0.7566 - val_loss: 0.7831 - val_accuracy: 0.3184\n",
      "Epoch 17/500\n",
      "21/21 [==============================] - 7s 328ms/step - loss: 0.4971 - accuracy: 0.7581 - val_loss: 0.7817 - val_accuracy: 0.3184\n",
      "Epoch 18/500\n",
      "21/21 [==============================] - 7s 328ms/step - loss: 0.4901 - accuracy: 0.7591 - val_loss: 0.7759 - val_accuracy: 0.3184\n",
      "Epoch 19/500\n",
      "21/21 [==============================] - 7s 327ms/step - loss: 0.4792 - accuracy: 0.7591 - val_loss: 0.7634 - val_accuracy: 0.3184\n",
      "Epoch 20/500\n",
      "21/21 [==============================] - 7s 328ms/step - loss: 0.4586 - accuracy: 0.7616 - val_loss: 0.7557 - val_accuracy: 0.3184\n",
      "Epoch 21/500\n",
      "21/21 [==============================] - 7s 325ms/step - loss: 0.4451 - accuracy: 0.7671 - val_loss: 0.7447 - val_accuracy: 0.3229\n",
      "Epoch 22/500\n",
      "21/21 [==============================] - 7s 326ms/step - loss: 0.4287 - accuracy: 0.7786 - val_loss: 0.7207 - val_accuracy: 0.4170\n",
      "Epoch 23/500\n",
      "21/21 [==============================] - 7s 328ms/step - loss: 0.4126 - accuracy: 0.8085 - val_loss: 0.6919 - val_accuracy: 0.5695\n",
      "Epoch 24/500\n",
      "21/21 [==============================] - 7s 325ms/step - loss: 0.3931 - accuracy: 0.8314 - val_loss: 0.6668 - val_accuracy: 0.6502\n",
      "Epoch 25/500\n",
      "21/21 [==============================] - 7s 329ms/step - loss: 0.3832 - accuracy: 0.8574 - val_loss: 0.6558 - val_accuracy: 0.6861\n",
      "Epoch 26/500\n",
      "21/21 [==============================] - 7s 331ms/step - loss: 0.3685 - accuracy: 0.8693 - val_loss: 0.6415 - val_accuracy: 0.7085\n",
      "Epoch 27/500\n",
      "21/21 [==============================] - 7s 327ms/step - loss: 0.3516 - accuracy: 0.8863 - val_loss: 0.6259 - val_accuracy: 0.7713\n",
      "Epoch 28/500\n",
      "21/21 [==============================] - 7s 330ms/step - loss: 0.3391 - accuracy: 0.8933 - val_loss: 0.6120 - val_accuracy: 0.7803\n",
      "Epoch 29/500\n",
      "21/21 [==============================] - 7s 328ms/step - loss: 0.3198 - accuracy: 0.9022 - val_loss: 0.6053 - val_accuracy: 0.7892\n",
      "Epoch 30/500\n",
      "21/21 [==============================] - 7s 328ms/step - loss: 0.3143 - accuracy: 0.9072 - val_loss: 0.5972 - val_accuracy: 0.7892\n",
      "Epoch 31/500\n",
      "21/21 [==============================] - 7s 326ms/step - loss: 0.3036 - accuracy: 0.9117 - val_loss: 0.5842 - val_accuracy: 0.7982\n",
      "Epoch 32/500\n",
      "21/21 [==============================] - 7s 324ms/step - loss: 0.2863 - accuracy: 0.9222 - val_loss: 0.5779 - val_accuracy: 0.7982\n",
      "Epoch 33/500\n",
      "21/21 [==============================] - 7s 325ms/step - loss: 0.2827 - accuracy: 0.9182 - val_loss: 0.5626 - val_accuracy: 0.8072\n",
      "Epoch 34/500\n",
      "21/21 [==============================] - 7s 328ms/step - loss: 0.2838 - accuracy: 0.9182 - val_loss: 0.5584 - val_accuracy: 0.8072\n",
      "Epoch 35/500\n",
      "21/21 [==============================] - 7s 327ms/step - loss: 0.2710 - accuracy: 0.9232 - val_loss: 0.5439 - val_accuracy: 0.7892\n",
      "Epoch 36/500\n",
      "21/21 [==============================] - 7s 324ms/step - loss: 0.2623 - accuracy: 0.9152 - val_loss: 0.5451 - val_accuracy: 0.7982\n",
      "Epoch 37/500\n",
      " 2/21 [=>............................] - ETA: 6s - loss: 0.2406 - accuracy: 0.9350"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# This is already separating in trainign and validation\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size = 100 # group of outtuples as a batch\n",
    "\n",
    "history = model.fit(\n",
    "    train_x_data,\n",
    "    train_y_data,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputfile = './rnn_model_2024-04-20.h5'\n",
    "model.summary()\n",
    "model.save(model_outputfile, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, \"ro\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"r\", label=\"Validation acc\")\n",
    "\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"test_results_acc.png\")\n",
    "\n",
    "plt.close()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"test_results_loss.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
